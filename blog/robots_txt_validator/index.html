<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <title>Robots.txt Validator – Test Robots.txt File for Blocking | SimpliConvert</title>
    <meta name="description" content="Analyze your robots.txt file to ensure you aren't accidentally blocking Google from indexing your most important pages. Test URL access against crawler rules instantly.">
    <meta name="keywords" content="robots.txt checker, seo crawler block, googlebot access check, robots tester, test robots.txt file for blocking, robots.txt validator">
    <meta name="author" content="SimpliConvert">
    <meta property="og:site_name" content="SimpliConvert">
    <meta name="robots" content="index, follow">

    <meta property="og:type" content="website">
    <meta property="og:url" content="https://simpliconvert.com/robots_txt_validator/">
    <meta property="og:title" content="Robots.txt Validator – Test Your SEO Crawler Rules">
    <meta property="og:description" content="Is your robots.txt file blocking the right pages? Use our validator to test URL access for Googlebot and other web crawlers.">
    <meta property="og:image" content="https://simpliconvert.com/images/robots-txt-validator-og.jpg">

    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:url" content="https://simpliconvert.com/robots_txt_validator/">
    <meta name="twitter:title" content="Robots.txt Validator – Check Googlebot Access">
    <meta name="twitter:description" content="Free tool to analyze robots.txt files. Ensure your site's most important pages are crawlable and indexed by search engines.">
    <meta name="twitter:image" content="https://simpliconvert.com/images/robots-txt-validator-og.jpg">

    <link rel="canonical" href="https://simpliconvert.com/robots_txt_validator/">

    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "WebApplication",
      "name": "Robots.txt Validator",
      "url": "https://simpliconvert.com/robots_txt_validator/",
      "description": "An online SEO tool to test and validate robots.txt files, ensuring that search engine crawlers like Googlebot can access the correct pages.",
      "applicationCategory": "SearchConsoleApplication",
      "operatingSystem": "Web Browser",
      "offers": {
        "@type": "Offer",
        "price": "0",
        "priceCurrency": "USD"
      },
      "publisher": {
        "@type": "Organization",
        "name": "SimpliConvert",
        "logo": {
          "@type": "ImageObject",
          "url": "https://simpliconvert.com/logo.png"
        }
      }
    }
    </script>

    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "BreadcrumbList",
      "itemListElement": [{
        "@type": "ListItem",
        "position": 1,
        "name": "Home",
        "item": "https://simpliconvert.com/"
      },{
        "@type": "ListItem",
        "position": 2,
        "name": "SEO Tools",
        "item": "https://simpliconvert.com/seo_tools/"
      },{
        "@type": "ListItem",
        "position": 3,
        "name": "Robots.txt Validator",
        "item": "https://simpliconvert.com/robots_txt_validator/"
      }]
    }
    </script>

    <script type="application/ld+json">
    {
      "@context": "https://schema.org",
      "@type": "FAQPage",
      "mainEntity": [{
        "@type": "Question",
        "name": "What does a robots.txt validator do?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "A robots.txt validator parses the directives in your robots.txt file (like Allow and Disallow) and checks them against a specific URL to see if a search engine crawler (like Googlebot) is permitted to access that page."
        }
      }, {
        "@type": "Question",
        "name": "Why is my URL showing as 'Blocked'?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "If a URL is blocked, it means there is a 'Disallow' directive in your robots.txt file that matches the path of that URL for the selected User-Agent. You should review your rules to ensure you aren't accidentally blocking important content."
        }
      }, {
        "@type": "Question",
        "name": "Does robots.txt stop a page from appearing in Google?",
        "acceptedAnswer": {
          "@type": "Answer",
          "text": "Not necessarily. Robots.txt prevents crawling, but if other sites link to the page, Google might still index the URL without knowing what's on the page. To fully prevent indexing, use a 'noindex' meta tag."
        }
      }]
    }
    </script>

    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css" />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
    
    <style>
        :root {
            --primary: #da3f0b;
            --primary-hover: #b53409;
            --primary-light: #fff5f2;
            --bg: #f8fafc;
            --text-dark: #0f172a;
            --success: #16a34a;
            --danger: #dc2626;
        }
        body { font-family: 'Inter', sans-serif; background-color: var(--bg); color: var(--text-dark); }
        .hero-card { background: white; border: 1px solid #f1f5f9; box-shadow: 0 10px 25px -5px rgba(0, 0, 0, 0.05); position: relative; overflow: hidden; }
        .hero-card::before { content: ''; position: absolute; top: 0; left: 0; width: 100%; height: 4px; background-color: var(--primary); }
        .animate-fade { animation: fadeIn 0.3s ease-out; }
        @keyframes fadeIn { from { opacity: 0; transform: translateY(8px); } to { opacity: 1; transform: translateY(0); } }
        .tool-card { transition: transform 0.2s ease, box-shadow 0.2s ease; }
        .tool-card:hover { transform: translateY(-4px); box-shadow: 0 10px 20px rgba(0,0,0,0.05); }
        .prose a { color: #da3f0b; text-decoration: none; }
        .prose a:hover { text-decoration: underline; }
        .code-editor { font-family: 'Courier New', Courier, monospace; line-height: 1.5; resize: none; }
    </style>
</head>

<body class="bg-gray-50 min-h-screen flex flex-col">
    <div id="navbar" style="min-height: 70px;"></div>
	
	<section class="hero-white text-black py-2 md:py-10">
        <div class="container mx-auto px-4">
            <nav class="flex pb-4 text-sm text-slate-500 items-center space-x-2 justify-center">
                <a href="https://simpliconvert.com/" class="hover:text-[#da3f0b] transition-colors">Home</a>
                <i class="fas fa-chevron-right text-[10px]"></i>
                <a href="https://simpliconvert.com/seo_tools/" class="hover:text-[#da3f0b] transition-colors">SEO Tools</a>
                <i class="fas fa-chevron-right text-[10px]"></i>
                <span class="text-slate-900 font-medium">Robots.txt Validator</span>
            </nav>
            <div class="text-center">
                <h1 class="text-3xl md:text-4xl lg:text-5xl font-bold mb-2">test robots.txt file for blocking</h1>
                <p class="text-lg md:text-xl max-w-3xl mx-auto mb-2 text-slate-600">
                    Ensure your website is crawlable. Paste your robots.txt content and test specific URLs to see if they are blocked by search engine bots.
                </p>
            </div>
        </div>
    </section>		

    <div id="validator-app" class="w-full max-w-6xl mx-auto p-4 sm:p-6">
        <div class="bg-white rounded-[2rem] shadow-2xl overflow-hidden border border-slate-100">
            <header class="bg-slate-50/50 border-b border-slate-100 p-6 md:p-8 flex flex-col md:flex-row justify-between items-center gap-6">
                <div class="text-center md:text-left">
                  <h2 class="text-2xl font-black text-slate-800 tracking-tight flex items-center justify-center md:justify-start gap-3">
                    <span class="bg-[#da3f0b] text-white p-2 rounded-xl text-lg shadow-lg shadow-[#da3f0b30]">
                      <i class="fas fa-robot"></i>
                    </span>
                    Robots.txt Directives Tester
                  </h2>
                </div>
                
                <div class="relative group min-w-[180px]">
                  <select id="user-agent-select" class="appearance-none w-full bg-white border border-slate-200 text-slate-700 py-2.5 pl-4 pr-10 rounded-xl text-xs font-bold focus:outline-none focus:ring-4 focus:ring-[#da3f0b15] focus:border-[#da3f0b] shadow-sm cursor-pointer hover:border-[#da3f0b40]">
                    <option value="*">All User-Agents (*)</option>
                    <option value="Googlebot">Googlebot</option>
                    <option value="Googlebot-Image">Googlebot-Image</option>
                    <option value="Bingbot">Bingbot</option>
                    <option value="Yandex">Yandex</option>
                    <option value="Baiduspider">Baiduspider</option>
                  </select>
                  <div class="pointer-events-none absolute inset-y-0 right-0 flex items-center px-3 text-slate-400">
                    <i class="fas fa-chevron-down text-[10px]"></i>
                  </div>
                </div>
            </header>
    
            <main class="flex flex-col lg:flex-row">
                <div class="lg:w-[60%] p-6 md:p-10 border-r border-slate-100">
                    <div class="space-y-6">
                        <div>
                            <label class="text-xs font-black text-slate-400 uppercase tracking-widest mb-3 block">Robots.txt Content</label>
                            <textarea id="robots-content" class="code-editor w-full h-64 p-4 bg-slate-900 text-green-400 rounded-2xl border border-slate-800 focus:ring-4 focus:ring-[#da3f0b20] focus:outline-none" placeholder="User-agent: *&#10;Disallow: /admin/&#10;Allow: /"></textarea>
                        </div>

                        <div>
                            <label class="text-xs font-black text-slate-400 uppercase tracking-widest mb-3 block">URL to Test</label>
                            <div class="relative rounded-xl shadow-sm">
                                <div class="absolute inset-y-0 left-0 pl-4 flex items-center pointer-events-none">
                                    <i class="fas fa-link text-slate-400"></i>
                                </div>
                                <input type="text" id="test-url" class="block w-full rounded-xl border-slate-200 border py-3 pl-11 pr-4 focus:ring-4 focus:ring-[#da3f0b20] focus:border-[#da3f0b] sm:text-sm font-semibold text-slate-800 transition-all bg-white" placeholder="/my-page-url">
                            </div>
                            <p class="text-[10px] text-slate-400 mt-2">Enter the path (e.g., /blog/post-1) or full URL.</p>
                        </div>

                        <button id="validate-btn" class="w-full bg-[#da3f0b] hover:bg-[#b53409] text-white font-bold py-4 px-6 rounded-xl shadow-lg shadow-[#da3f0b30] transition-all transform active:scale-[0.98] flex items-center justify-center gap-3">
                            <i class="fas fa-search"></i> Check Access
                        </button>
                    </div>
                </div>
    
                <div class="lg:w-[40%] bg-slate-50/50 p-6 md:p-10 flex flex-col justify-center">
                   <div id="result-container" class="space-y-6 hidden animate-fade">
                     
                     <div id="status-card" class="hero-card p-8 flex flex-col items-center text-center">
                         <p class="text-xs font-black uppercase tracking-[0.2em] mb-4 text-slate-500">Crawl Status</p>
                         <div id="status-icon" class="text-6xl mb-4"></div>
                         <h3 id="status-text" class="text-3xl font-black tracking-tight">ALLOWED</h3>
                         <p id="status-desc" class="text-sm text-slate-500 font-medium mt-4"></p>
                     </div>
    
                     <div class="bg-white p-6 rounded-2xl border border-slate-100 shadow-sm">
                        <h4 class="text-xs font-black text-slate-400 uppercase tracking-widest mb-4">Matching Rule</h4>
                        <div id="matching-rule" class="p-3 bg-slate-50 rounded-lg border border-slate-100 font-mono text-sm text-slate-700 break-all">
                            No specific rule matched.
                        </div>
                     </div>

                     <div class="bg-white p-6 rounded-2xl border border-slate-100 shadow-sm">
                        <h4 class="text-xs font-black text-slate-400 uppercase tracking-widest mb-4">Quick Insights</h4>
                        <ul class="space-y-3 text-xs font-medium text-slate-600">
                            <li class="flex items-center gap-2"><i class="fas fa-info-circle text-[#da3f0b]"></i> <span id="insight-1">User-agent rules are case-sensitive.</span></li>
                            <li class="flex items-center gap-2"><i class="fas fa-info-circle text-[#da3f0b]"></i> <span id="insight-2">Allow takes precedence over Disallow in modern bots.</span></li>
                        </ul>
                     </div>
                   </div>

                   <div id="empty-state" class="text-center py-12">
                        <div class="w-20 h-20 bg-slate-100 rounded-full flex items-center justify-center mx-auto mb-4">
                            <i class="fas fa-terminal text-slate-300 text-3xl"></i>
                        </div>
                        <h3 class="text-slate-800 font-bold">Ready to Validate</h3>
                        <p class="text-slate-500 text-sm max-w-[200px] mx-auto mt-2">Enter your robots.txt content and a URL to see the results here.</p>
                   </div>
                </div>
            </main>
    
            <footer class="bg-slate-50 border-t border-slate-100 p-6 md:p-8">
               <div class="flex flex-col md:flex-row items-center justify-between gap-6">
                   <div class="text-center md:text-left">
                       <h4 class="text-sm font-bold text-slate-700 mb-1">Share this SEO Tool</h4>
                       <p class="text-xs text-slate-400">Help webmasters optimize their crawling.</p>
                   </div>
                   
                   <div class="flex flex-wrap justify-center gap-3">
                       <a href="#" onclick="shareSocial('facebook')" class="w-10 h-10 rounded-full flex items-center justify-center text-white transition-transform hover:-translate-y-1 hover:shadow-lg bg-[#1877F2]"><i class="fab fa-facebook-f"></i></a>
                       <a href="#" onclick="shareSocial('twitter')" class="w-10 h-10 rounded-full flex items-center justify-center text-white transition-transform hover:-translate-y-1 hover:shadow-lg bg-[#1DA1F2]"><i class="fab fa-twitter"></i></a>
                       <a href="#" onclick="shareSocial('linkedin')" class="w-10 h-10 rounded-full flex items-center justify-center text-white transition-transform hover:-translate-y-1 hover:shadow-lg bg-[#0A66C2]"><i class="fab fa-linkedin-in"></i></a>
                       <a href="#" onclick="shareSocial('whatsapp')" class="w-10 h-10 rounded-full flex items-center justify-center text-white transition-transform hover:-translate-y-1 hover:shadow-lg bg-[#25D366]"><i class="fab fa-whatsapp"></i></a>
                   </div>
    
                   <div class="relative">
                       <button onclick="handleBookmark()" class="flex items-center gap-2 bg-white border border-slate-200 hover:border-[#da3f0b] hover:text-[#da3f0b] text-slate-600 px-4 py-2 rounded-lg text-sm font-bold transition-all shadow-sm active:scale-95">
                          <i class="fas fa-bookmark"></i> Bookmark
                       </button>
                       <div id="bookmark-toast" class="hidden absolute bottom-full mb-3 left-1/2 -translate-x-1/2 bg-slate-800 text-white text-xs py-2 px-4 rounded-lg shadow-lg whitespace-nowrap animate-fade">
                           Press <span class="font-bold text-[#da3f0b]">Ctrl + D</span> to bookmark
                       </div>
                   </div>
               </div>
            </footer>
        </div>
    </div>

    <script>
    document.addEventListener('DOMContentLoaded', function() {
        const robotsInput = document.getElementById('robots-content');
        const urlInput = document.getElementById('test-url');
        const uaSelect = document.getElementById('user-agent-select');
        const validateBtn = document.getElementById('validate-btn');
        const resultContainer = document.getElementById('result-container');
        const emptyState = document.getElementById('empty-state');
        
        const statusIcon = document.getElementById('status-icon');
        const statusText = document.getElementById('status-text');
        const statusDesc = document.getElementById('status-desc');
        const matchingRule = document.getElementById('matching-rule');

        // Set default content
        robotsInput.value = "User-agent: *\nDisallow: /cgi-bin/\nDisallow: /tmp/\nDisallow: /private/\n\nUser-agent: Googlebot\nDisallow: /no-google/\nAllow: /no-google/public/";

        validateBtn.addEventListener('click', function() {
            const robotsTxt = robotsInput.value;
            const testUrl = urlInput.value.trim() || "/";
            const userAgent = uaSelect.value;

            if (!robotsTxt) {
                alert("Please enter robots.txt content.");
                return;
            }

            const result = checkRobots(robotsTxt, userAgent, testUrl);
            displayResult(result, testUrl, userAgent);
        });

        function checkRobots(content, targetUA, url) {
            const lines = content.split('\n');
            let currentUAs = [];
            let rules = []; // { ua, type, pattern }

            // Parse robots.txt
            lines.forEach(line => {
                line = line.split('#')[0].trim(); // Remove comments
                if (!line) return;

                const parts = line.split(':');
                if (parts.length < 2) return;

                const key = parts[0].trim().toLowerCase();
                const value = parts.slice(1).join(':').trim();

                if (key === 'user-agent') {
                    // If we were already collecting UAs and now see a new UA block, 
                    // it means the previous block ended. But standard parsing 
                    // usually groups consecutive UA lines.
                    if (rules.length > 0 && lines[lines.indexOf(line)-1] && !lines[lines.indexOf(line)-1].toLowerCase().startsWith('user-agent')) {
                        // This logic is simplified; standard robots.txt parsing is block-based.
                    }
                    currentUAs.push(value.toLowerCase());
                } else if (key === 'disallow' || key === 'allow') {
                    currentUAs.forEach(ua => {
                        rules.push({ ua, type: key, pattern: value });
                    });
                } else {
                    // Reset currentUAs if we hit a non-UA/Allow/Disallow line after a block
                    // Actually, standard parsing: a block is User-agent lines followed by Allow/Disallow.
                    // Once we hit Allow/Disallow, any subsequent User-agent starts a NEW block.
                }
                
                // Reset UAs if we just finished a block of rules and hit a new User-agent
                if (key !== 'user-agent' && currentUAs.length > 0) {
                    // We keep currentUAs until we hit the next 'user-agent' key
                }
            });

            // Re-parsing logic to handle blocks correctly
            let blocks = [];
            let activeBlock = { uas: [], rules: [] };
            
            lines.forEach(line => {
                line = line.split('#')[0].trim();
                if (!line) return;
                const parts = line.split(':');
                const key = parts[0].trim().toLowerCase();
                const value = parts.slice(1).join(':').trim();

                if (key === 'user-agent') {
                    if (activeBlock.rules.length > 0) {
                        blocks.push(activeBlock);
                        activeBlock = { uas: [], rules: [] };
                    }
                    activeBlock.uas.push(value.toLowerCase());
                } else if (key === 'allow' || key === 'disallow') {
                    activeBlock.rules.push({ type: key, pattern: value });
                }
            });
            blocks.push(activeBlock);

            // Find the best block for our targetUA
            let targetBlock = blocks.find(b => b.uas.includes(targetUA.toLowerCase()));
            if (!targetBlock && targetUA !== '*') {
                targetBlock = blocks.find(b => b.uas.includes('*'));
            }
            if (!targetBlock) return { allowed: true, rule: "Default (No rules found)" };

            // Check URL against rules in the block
            // Sort rules by pattern length (longest match wins in Google's logic)
            const sortedRules = targetBlock.rules.sort((a, b) => b.pattern.length - a.pattern.length);
            
            for (let rule of sortedRules) {
                if (matchPattern(url, rule.pattern)) {
                    return { 
                        allowed: rule.type === 'allow', 
                        rule: `${rule.type.toUpperCase()}: ${rule.pattern}`,
                        ua: targetBlock.uas.join(', ')
                    };
                }
            }

            return { allowed: true, rule: "Default (No matching Disallow found)" };
        }

        function matchPattern(url, pattern) {
            if (!pattern) return false;
            // Escape regex chars except *
            let regexStr = pattern.replace(/[.+^${}()|[\]\\]/g, '\\$&').replace(/\*/g, '.*');
            if (!regexStr.endsWith('$')) {
                // Robots.txt matches prefix unless anchored with $
            } else {
                regexStr = regexStr.slice(0, -1) + '$';
            }
            const regex = new RegExp('^' + regexStr);
            return regex.test(url);
        }

        function displayResult(result, url, ua) {
            emptyState.classList.add('hidden');
            resultContainer.classList.remove('hidden');

            if (result.allowed) {
                statusCard.style.borderColor = '#16a34a';
                statusIcon.innerHTML = '<i class="fas fa-check-circle text-[#16a34a]"></i>';
                statusText.innerText = "ALLOWED";
                statusText.style.color = "#16a34a";
                statusDesc.innerText = `Search engines (${ua}) can crawl "${url}".`;
            } else {
                statusCard.style.borderColor = '#dc2626';
                statusIcon.innerHTML = '<i class="fas fa-ban text-[#dc2626]"></i>';
                statusText.innerText = "BLOCKED";
                statusText.style.color = "#dc2626";
                statusDesc.innerText = `Search engines (${ua}) are restricted from crawling "${url}".`;
            }

            matchingRule.innerText = result.rule;
        }

        window.shareSocial = function(platform) {
             const url = encodeURIComponent(window.location.href);
             const title = encodeURIComponent("Check out this Robots.txt Validator!");
             let shareUrl = '';
             switch(platform) {
                case 'facebook': shareUrl = `https://www.facebook.com/sharer/sharer.php?u=${url}`; break;
                case 'twitter': shareUrl = `https://twitter.com/intent/tweet?url=${url}&text=${title}`; break;
                case 'linkedin': shareUrl = `https://www.linkedin.com/shareArticle?mini=true&url=${url}`; break;
                case 'whatsapp': shareUrl = `https://api.whatsapp.com/send?text=${title}%20${url}`; break;
             }
             if(shareUrl) window.open(shareUrl, '_blank');
        };

        window.handleBookmark = function() {
            const toast = document.getElementById('bookmark-toast');
            toast.classList.remove('hidden');
            setTimeout(() => toast.classList.add('hidden'), 3000);
        };
    });
    </script>

    <section class="bg-white py-12 sm:py-16">
        <div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="prose prose-lg max-w-none prose-h2:text-gray-800 prose-p:text-gray-700">
                <h2 class="text-3xl font-bold text-gray-900 mb-4">Why You Should Test Robots.txt File for Blocking</h2>
                <p class="mb-6 leading-relaxed">
                    The robots.txt file is one of the most critical components of your technical SEO strategy. It acts as a gatekeeper, telling search engine crawlers like Googlebot, Bingbot, and others which parts of your site they are allowed to visit. However, a single typo or an overly broad "Disallow" directive can accidentally hide your most valuable content from search results. Using a <strong>robots.txt validator</strong> is the best way to ensure your site remains visible.
                </p>

                <h2 class="text-2xl font-bold mt-8 mb-4">How the Robots.txt Checker Works</h2>
                <p class="mb-6 leading-relaxed">
                    Our tool simulates how a search engine bot reads your directives. By parsing the "User-agent", "Allow", and "Disallow" lines, it determines the accessibility of any specific URL path. This is particularly useful when you are implementing new site sections or trying to hide sensitive directories like <code>/admin/</code> or <code>/temp/</code>. 
                </p>
                <p class="mb-6 leading-relaxed">
                    For a complete SEO audit, we recommend using this tool alongside our <a href="https://simpliconvert.com/sitemap_validator/">Sitemap Validator</a> to ensure that the URLs you want indexed are both crawlable and correctly listed in your XML sitemap.
                </p>

                <h2 class="text-2xl font-bold mt-8 mb-4">Common Robots.txt Mistakes to Avoid</h2>
                <ul class="list-disc pl-6 mb-6 space-y-2 text-gray-700">
                    <li><strong>Blocking CSS and JS:</strong> Modern crawlers need access to these files to understand your page layout and mobile-friendliness.</li>
                    <li><strong>Trailing Slashes:</strong> <code>Disallow: /blog</code> blocks both the folder and any file starting with "blog", whereas <code>Disallow: /blog/</code> only blocks the folder.</li>
                    <li><strong>Case Sensitivity:</strong> Robots.txt directives are case-sensitive. <code>/Admin/</code> is not the same as <code>/admin/</code>.</li>
                    <li><strong>Conflicting Rules:</strong> If you have both an Allow and a Disallow rule for the same path, modern bots usually favor the more specific rule or the "Allow" directive.</li>
                </ul>

                <h2 class="text-2xl font-bold mt-8 mb-4">Integrating with Other SEO Tools</h2>
                <p class="mb-6 leading-relaxed">
                    Validating your robots.txt is just the first step. Once you've confirmed that Googlebot can access your pages, you should ensure your on-page SEO is optimized. Use our <a href="https://simpliconvert.com/meta_tag_generator/">Meta Tag Generator</a> to create perfect titles and descriptions, and don't forget to check your <a href="https://simpliconvert.com/canonical_url_generator/">Canonical URL Generator</a> to prevent duplicate content issues.
                </p>

                <h2 class="text-2xl font-bold mt-8 mb-4">Final Thoughts on Crawler Access</h2>
                <p class="mb-6 leading-relaxed">
                    Remember that robots.txt is a request, not a command. While reputable bots like Googlebot respect these rules, malicious scrapers may ignore them. Furthermore, if a page is blocked by robots.txt but has many external links, it might still appear in search results (though without a description). To completely hide a page, use the <code>noindex</code> meta tag instead.
                </p>
            </div>
        </div>
    </section>

    <section class="bg-gray-50 py-12 sm:py-16">
        <div class="max-w-4xl mx-auto px-4 sm:px-6 lg:px-8">
            <h2 class="text-3xl font-bold text-center text-gray-900 mb-8">Robots.txt FAQ</h2>
            <div class="space-y-4">
                <div class="bg-white rounded-lg shadow-sm">
                    <button data-faq-toggle class="w-full flex justify-between items-center text-left p-5 font-semibold text-gray-800 hover:bg-gray-100 rounded-lg focus:outline-none">
                        <span>What is the 'User-agent: *' directive?</span>
                        <i class="fas fa-chevron-down faq-icon transition-transform"></i>
                    </button>
                    <div class="faq-answer max-h-0 overflow-hidden transition-all duration-300">
                        <p class="p-5 pt-0 text-gray-600">The asterisk (*) is a wildcard that applies the following rules to all search engine crawlers that don't have a specific block of their own in the file.</p>
                    </div>
                </div>
                <div class="bg-white rounded-lg shadow-sm">
                    <button data-faq-toggle class="w-full flex justify-between items-center text-left p-5 font-semibold text-gray-800 hover:bg-gray-100 rounded-lg focus:outline-none">
                        <span>Can I block specific images from Google?</span>
                        <i class="fas fa-chevron-down faq-icon transition-transform"></i>
                    </button>
                    <div class="faq-answer max-h-0 overflow-hidden transition-all duration-300">
                        <p class="p-5 pt-0 text-gray-600">Yes, you can use 'User-agent: Googlebot-Image' followed by a 'Disallow' rule for specific image paths or file types like .jpg or .png.</p>
                    </div>
                </div>
                <div class="bg-white rounded-lg shadow-sm">
                    <button data-faq-toggle class="w-full flex justify-between items-center text-left p-5 font-semibold text-gray-800 hover:bg-gray-100 rounded-lg focus:outline-none">
                        <span>How often does Google check robots.txt?</span>
                        <i class="fas fa-chevron-down faq-icon transition-transform"></i>
                    </button>
                    <div class="faq-answer max-h-0 overflow-hidden transition-all duration-300">
                        <p class="p-5 pt-0 text-gray-600">Google typically caches your robots.txt file for up to 24 hours. If you make urgent changes, you can use the 'Robots.txt Tester' in Google Search Console to ask for a recrawl.</p>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <section class="bg-gray-50 pb-16">
        <div class="max-w-6xl mx-auto px-4 sm:px-6 lg:px-8 border-t border-slate-200 pt-16">
            <h2 class="text-2xl font-bold text-center text-slate-800 mb-10">Popular SEO Tools</h2>
            <div class="grid grid-cols-2 md:grid-cols-3 lg:grid-cols-4 gap-4 sm:gap-6">
                <a href="/meta_tag_generator/" class="tool-card bg-white p-4 rounded-xl border border-slate-100 hover:border-[#da3f0b] hover:shadow-md transition-all group flex items-start gap-4">
                    <div class="w-10 h-10 rounded-lg bg-indigo-50 text-indigo-600 flex items-center justify-center shrink-0 group-hover:bg-indigo-600 group-hover:text-white transition-colors">
                        <i class="fas fa-search text-lg"></i>
                    </div>
                    <div>
                        <h3 class="font-bold text-slate-800 text-sm mb-1 group-hover:text-[#da3f0b]">Meta Tag Gen</h3>
                        <p class="text-xs text-slate-500 leading-snug">Optimize SEO with custom tags.</p>
                    </div>
                </a>
                <a href="/sitemap_validator/" class="tool-card bg-white p-4 rounded-xl border border-slate-100 hover:border-[#da3f0b] hover:shadow-md transition-all group flex items-start gap-4">
                    <div class="w-10 h-10 rounded-lg bg-blue-50 text-blue-600 flex items-center justify-center shrink-0 group-hover:bg-blue-600 group-hover:text-white transition-colors">
                        <i class="fas fa-sitemap text-lg"></i>
                    </div>
                    <div>
                        <h3 class="font-bold text-slate-800 text-sm mb-1 group-hover:text-[#da3f0b]">Sitemap Check</h3>
                        <p class="text-xs text-slate-500 leading-snug">Validate your XML sitemaps.</p>
                    </div>
                </a>
                <a href="/canonical_url_generator/" class="tool-card bg-white p-4 rounded-xl border border-slate-100 hover:border-[#da3f0b] hover:shadow-md transition-all group flex items-start gap-4">
                    <div class="w-10 h-10 rounded-lg bg-red-50 text-red-600 flex items-center justify-center shrink-0 group-hover:bg-red-600 group-hover:text-white transition-colors">
                        <i class="fas fa-link text-lg"></i>
                    </div>
                    <div>
                        <h3 class="font-bold text-slate-800 text-sm mb-1 group-hover:text-[#da3f0b]">Canonical Gen</h3>
                        <p class="text-xs text-slate-500 leading-snug">Prevent duplicate content.</p>
                    </div>
                </a>
                <a href="/title_length_checker/" class="tool-card bg-white p-4 rounded-xl border border-slate-100 hover:border-[#da3f0b] hover:shadow-md transition-all group flex items-start gap-4">
                    <div class="w-10 h-10 rounded-lg bg-green-50 text-green-600 flex items-center justify-center shrink-0 group-hover:bg-green-600 group-hover:text-white transition-colors">
                        <i class="fas fa-heading text-lg"></i>
                    </div>
                    <div>
                        <h3 class="font-bold text-slate-800 text-sm mb-1 group-hover:text-[#da3f0b]">Title Checker</h3>
                        <p class="text-xs text-slate-500 leading-snug">Check SERP title lengths.</p>
                    </div>
                </a>
                <a href="/meta_description_length_checker/" class="tool-card bg-white p-4 rounded-xl border border-slate-100 hover:border-[#da3f0b] hover:shadow-md transition-all group flex items-start gap-4">
                    <div class="w-10 h-10 rounded-lg bg-orange-50 text-orange-600 flex items-center justify-center shrink-0 group-hover:bg-orange-600 group-hover:text-white transition-colors">
                        <i class="fas fa-align-left text-lg"></i>
                    </div>
                    <div>
                        <h3 class="font-bold text-slate-800 text-sm mb-1 group-hover:text-[#da3f0b]">Meta Desc Check</h3>
                        <p class="text-xs text-slate-500 leading-snug">Optimize snippet descriptions.</p>
                    </div>
                </a>
                <a href="/hreflang_generator/" class="tool-card bg-white p-4 rounded-xl border border-slate-100 hover:border-[#da3f0b] hover:shadow-md transition-all group flex items-start gap-4">
                    <div class="w-10 h-10 rounded-lg bg-purple-50 text-purple-600 flex items-center justify-center shrink-0 group-hover:bg-purple-600 group-hover:text-white transition-colors">
                        <i class="fas fa-language text-lg"></i>
                    </div>
                    <div>
                        <h3 class="font-bold text-slate-800 text-sm mb-1 group-hover:text-[#da3f0b]">Hreflang Gen</h3>
                        <p class="text-xs text-slate-500 leading-snug">Generate multi-language tags.</p>
                    </div>
                </a>
            </div>
        </div>
    </section>

    <div id="footer"></div>

    <script>
    async function loadComponent(url, elementId, callback) {
        try {
            const response = await fetch(url);
            const text = await response.text();
            const element = document.getElementById(elementId);
            if (element) {
                element.innerHTML = text;
                if (callback) callback();
            }
        } catch (error) {
            console.error('Error loading component:', error);
        }
    }

    document.addEventListener('DOMContentLoaded', function() {
        loadComponent('/components/navbar.html', 'navbar');
        loadComponent('/components/footer.html', 'footer', () => {
            const yearSpan = document.getElementById('current-year');
            if(yearSpan) yearSpan.textContent = new Date().getFullYear();
        });
        
        const faqToggles = document.querySelectorAll('[data-faq-toggle]');
        faqToggles.forEach(button => {
            button.addEventListener('click', () => {
                const answer = button.nextElementSibling;
                const icon = button.querySelector('.faq-icon');
                const isOpen = answer.style.maxHeight;

                faqToggles.forEach(other => {
                    other.nextElementSibling.style.maxHeight = null;
                    other.querySelector('.faq-icon').style.transform = 'rotate(0deg)';
                });

                if (!isOpen) {
                    answer.style.maxHeight = answer.scrollHeight + "px";
                    icon.style.transform = 'rotate(180deg)';
                }
            });
        });
    });
    </script>
</body>
</html>