<!DOCTYPE html>
<html lang="en">
<head>
  <!-- Primary Meta Tags -->
<meta name="title" content="robots.txt Generator – Create robots.txt File Free Online | SimpliConvert">
<meta name="description" content="Generate a perfect robots.txt file instantly. Control search engine crawlers, block bots, and optimize your crawl budget. Free tool for WordPress, Shopify, and all websites.">
<meta name="keywords" content="robots.txt generator, robots.txt file generator, create robots.txt online, robots.txt maker, free robots.txt tool, robots.txt generator for WordPress, robots.txt disallow all generator, robots.txt syntax, sitemap location, user-agent, crawl delay, robots.txt tester, robots.txt validator">
<meta name="author" content="SimpliConvert">
<meta name="robots" content="index, follow">

<!-- Open Graph / Facebook -->
<meta property="og:type" content="website">
<meta property="og:url" content="https://simpliconvert.com/robots-txt-generator/">
<meta property="og:title" content="Free robots.txt Generator – Control Search Engine Crawlers">
<meta property="og:description" content="Create a properly formatted robots.txt file to manage search engine crawling. Block bots, set crawl delays, and point to your sitemap. Essential SEO tool.">
<meta property="og:image" content="https://simpliconvert.com/images/robots-txt-generator-og.jpg">

<!-- Twitter -->
<meta property="twitter:card" content="summary_large_image">
<meta property="twitter:url" content="https://simpliconvert.com/robots-txt-generator/">
<meta property="twitter:title" content="robots.txt Generator – Free SEO Tool for Website Crawl Control">
<meta property="twitter:description" content="Generate robots.txt files that protect your server and guide search engines. Block AI crawlers, prevent duplicate content issues, and optimize crawl budget.">
<meta property="twitter:image" content="https://simpliconvert.com/images/robots-txt-generator-og.jpg">

<!-- Canonical URL -->
<link rel="canonical" href="https://simpliconvert.com/robots-txt-generator/">
<title>robots.txt Generator – Create robots.txt File Free Online | SimpliConvert</title>

<!-- Schema.org Structured Data -->
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "WebApplication",
  "name": "robots.txt Generator",
  "url": "https://simpliconvert.com/robots-txt-generator/",
  "description": "Free online tool to generate robots.txt files for SEO. Control search engine crawlers, block bots, and manage crawl budget for WordPress and other websites.",
  "applicationCategory": "SEOApplication",
  "operatingSystem": "Web Browser",
  "offers": {
    "@type": "Offer",
    "price": "0",
    "priceCurrency": "USD"
  },
  "publisher": {
    "@type": "Organization",
    "name": "SimpliConvert",
    "logo": {
      "@type": "ImageObject",
      "url": "https://simpliconvert.com/logo.png"
    }
  }
}
</script>
  
    <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700;800&display=swap" rel="stylesheet">
  
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: {
            primary: '#da3f0b',
            primaryLight: '#f57648',
          },
          fontFamily: {
            sans: ['Inter', 'system-ui', 'sans-serif'],
          },
        }
      }
    }
  </script>
  



<style>

    /* Custom styles for interactive elements */
    .faq-item[open] .faq-icon {
      transform: rotate(180deg);
    }
        body {
            font-family: 'Inter', sans-serif;
            -webkit-font-smoothing: antialiased;
            -moz-osx-font-smoothing: grayscale;
        }

        /* Header Scroll Effect */
        .scrolled-effect {
            background-color: rgba(255, 255, 255, 0.8) !important;
            box-shadow: 0 4px 6px -1px rgb(0 0 0 / 0.1), 0 2px 4px -2px rgb(0 0 0 / 0.1) !important;
            backdrop-filter: blur(8px);
        }

        /* Mega Menu & Mobile Styles */
        #mega-menu { opacity: 0; visibility: hidden; transform: translateY(-1rem); transition: all 0.3s ease-in-out; pointer-events: none; right: 0; left: auto; transform-origin: top right; min-width: 800px; width: auto; position: absolute; top: 100%; z-index: 1000; }
        #mega-menu-trigger:hover > #mega-menu, #mega-menu:hover { opacity: 1; visibility: visible; transform: translateY(0); pointer-events: auto; }
        #mega-menu-trigger { position: relative; }
        #mobile-menu { max-height: 0; overflow: hidden; transition: max-height 0.3s ease-in-out; }
        #mobile-menu.open { max-height: 80vh; box-shadow: 0 10px 15px -3px rgb(0 0 0 / 0.1), 0 4px 6px -4px rgb(0 0 0 / 0.1); }
        .accordion-content { max-height: 0; overflow: hidden; transition: max-height 0.3s ease-in-out; }
        .accordion-content.open { max-height: 500px; }
        .accordion-button.open svg { transform: rotate(180deg); }
        .nav-dropdown { opacity: 0; visibility: hidden; transform: translateY(15px); transition: all 0.3s ease; }
        .nav-item:hover .nav-dropdown { opacity: 1; visibility: visible; transform: translateY(0); }
        .footer-link:hover { background-color: rgba(255,255,255,0.1); }

        /* Hero Gradient */
        .hero-gradient { background: linear-gradient(135deg, #da3f0b 0%, #f57648 100%); }
    </style>



  
</head>
<body class="bg-gray-50 font-sans text-gray-900">
  
     <div id="navbar"></div>

  <main class="container mx-auto px-4 py-8 max-w-4xl">
    <article class="bg-white rounded-xl shadow-lg overflow-hidden">
      
            <header class="p-8 md:p-12 border-b">
        <div class="flex items-center text-sm text-gray-500 mb-4 space-x-2">
          <time datetime="2025-11-15">November 15, 2025</time>
          <span>•</span>
          <span>8 min read</span>
          <span>•</span>
          <a href="https://simpliconvert.com/tools/seo-analyzer/" class="text-primary hover:underline">CHECK OUR SEO TOOLS</a>
        </div>
        
        <h1 class="text-4xl md:text-5xl font-bold mb-6 leading-tight">
          The Day I Accidentally Blocked Google (And Why Every Website Needs a Bulletproof robots.txt)
        </h1>
        
        <p class="text-xl text-gray-600 leading-relaxed">
          The email landed at 2:47 AM: "Site completely gone from Google." The cause? A single line in a file most owners ignore. Here’s how to make sure your robots.txt file is an SEO weapon, not a site-killer.
        </p>
        
        <div class="mt-8 flex items-center">
          <img src="/images/author-avatar-seo.jpg" alt="Author" class="w-12 h-12 rounded-full mr-4">
          <div>
            <p class="font-semibold">Alex Chen</p>
            <p class="text-sm text-gray-500">Technical SEO Specialist</p>
          </div>
        </div>
      </header>

            <figure class="w-full">
        <img src="https://simpliconvert.com/images/robots-txt-seo-disaster.jpg" 
             alt="Illustration of a website with a 'Closed' sign being blocked from a Google search crawler" 
             class="w-full h-auto object-cover"
             loading="lazy">
        <figcaption class="text-center text-sm text-gray-500 p-4 bg-gray-50">
          A simple robots.txt file is the bouncer at your website's front door; getting it wrong is like accidentally putting up a "Closed Forever" sign.
        </figcaption>
      </figure>

            <div class="prose prose-lg max-w-none p-8 md:p-12 text-gray-700">
        
        <p class="text-xl leading-relaxed text-gray-800 font-medium mb-6">
          The email landed in my inbox at 2:47 AM, and I knew it was bad before I even opened it. Subject line: "URGENT - Site completely gone from Google."
        </p>
        <p>
          The client was a major e-commerce store, and they'd watched their organic traffic flatline to zero over 48 hours. My first thought was a manual penalty. My second was a hacking incident. The actual cause? A single line in their robots.txt file that some well-meaning developer had added during a "routine update."
        </p>

        <pre><code>Disallow: /</code></pre>

        <p>That's it. That's what nuked their entire search presence. Google's crawler hit that line, shrugged, and said "well, guess I'm not wanted here," and poof—three years of SEO work evaporated. We fixed it in thirty seconds, but it took three weeks to get fully re-indexed and another two months to recover their rankings. All because of a file most website owners don't even know exists.</p>

        <p>If you're reading this and thinking "wait, what's a robots.txt file?"—don't worry, you're not alone. I've consulted with Fortune 500 companies where the dev team has never touched it. But here's the thing: this little text file is the bouncer at your website's front door. It tells search engines what they can and cannot look at, and getting it wrong is like accidentally putting a "Closed Forever" sign on your business.</p>

        <h2>What This File Actually Does (And Why It's Your Secret SEO Weapon)</h2>
        
        <p>In the simplest terms, robots.txt is a set of instructions for web crawlers. You put it in your root directory, and when Google's bot (or Bing's, or any legitimate crawler) arrives, it reads this file first. Then it decides what to crawl and what to skip.</p>

        <p>But here's where it gets interesting—and where most people mess up. The robots.txt file doesn't actually enforce anything. It's a suggestion, not a lock. Malicious bots ignore it completely. It's like a "No Trespassing" sign: honest people respect it, thieves don't. So while it's crucial for SEO, it's useless for security, a misconception I have to correct weekly.</p>

        <p>The real power of robots.txt isn't in blocking everything; it's in strategic guidance. I have a client with a massive video library—terabytes of content. Their bandwidth bills were astronomical because search bots were crawling every single video file, every day, even though those videos never appeared in search results. One line in robots.txt: <code>Disallow: /*.mp4$</code> and their server load dropped by 40%. That translates to real money.</p>

        <h2>The Architecture of a Perfect robots.txt File</h2>
        
        <p>Most generators spit out something like this:</p>

        <pre><code>User-agent: *
Disallow: /wp-admin/
Allow: /wp-admin/admin-ajax.php</code></pre>

        <p>And that's fine. It's functional. But it's also the bare minimum, like showing up to a marathon in flip-flops. You can technically run, but you're not going to win.</p>
        <p>Here's what I consider the gold standard structure:</p>

        <pre><code>User-agent: *
Crawl-delay: 1
Disallow: /wp-admin/
Disallow: /wp-includes/
Disallow: /cgi-bin/
Disallow: /cart/
Disallow: /checkout/
Disallow: /my-account/
Allow: /wp-admin/admin-ajax.php

User-agent: Googlebot
Allow: *.css
Allow: *.js

User-agent: Googlebot-Image
Disallow: /private-images/

Sitemap: https://yoursite.com/sitemap.xml
Sitemap: https://yoursite.com/post-sitemap.xml</code></pre>

        <p>See the difference? We're not just blocking admin areas; we're managing crawl budget, prioritizing Google-specific access, and explicitly pointing to our sitemaps. Each line has a purpose.</p>

        <h2>Why "Set It and Forget It" Is a Dangerous Myth</h2>
        
        <p>The robots.txt file is a living document. At least, it should be. I review mine quarterly, and more often during site redesigns. Why? Because things change.</p>
        <p>Last year, WordPress released a major update that changed how their REST API endpoints worked. Sites that had explicitly allowed certain paths suddenly found those paths had moved. Their robots.txt was now blocking critical functionality. It wasn't WordPress's fault—it was the site owner's responsibility to keep that file current.</p>
        <p>I also track crawler behavior. In Google Search Console, there's a beautiful report that shows you exactly which pages Google is crawling and how often. If I see Google hitting a bunch of URL parameters that are creating infinite crawl traps (?filter=red, ?filter=blue, ?filter=red&sort=price), I'll add a line to block those patterns.</p>
        <p>The issue is that most businesses treat robots.txt like their attic: set it up once, never look at it again. But unlike your attic, this thing is being accessed thousands of times a day and has direct impact on your revenue.</p>

        <h2>Real-World Disasters (And How to Avoid Them)</h2>
        
        <p>Let me share a few more war stories, because sometimes the best way to learn is through someone else's pain.</p>

        <h3>The Staging Site Nightmare</h3>
        <p>A development team I worked with had their staging site (staging.company.com) publicly accessible but blocked in robots.txt. Smart, right? Except they'd copied the robots.txt from production, which included the production sitemap. Google found the staging site through the sitemap, indexed it, and suddenly they had duplicate content competing with their main site. The fix was simple—a separate robots.txt for staging that didn't reference the sitemap—but the damage took months to clean up.</p>

        <h3>The CMS Plugin Fiasco</h3>
        <p>Yoast SEO, Rank Math, All in One SEO—all great plugins. They all have a feature to "write robots.txt for you." Handy, until you have three plugins on three different subdomains all writing conflicting rules. I've seen sites where one plugin allowed a path while another plugin blocked it. The result? Unpredictable crawling that tanked indexation.</p>

        <h3>The Eager Marketing Manager</h3>
        <p>A marketing director decided to block all crawlers from the blog section while they "revamped the content." They added <code>Disallow: /blog/</code> and then forgot about it for six months. That blog had built up significant authority over two years. When they finally removed the line, Google treated it like a new section, not a restored one. They lost all their rankings and had to start from scratch.</p>

        <h2>My Current robots.txt Strategy for 2025</h2>
        
        <p>Given how AI crawlers are now entering the scene—OpenAI's GPTBot, Claude's crawler, etc.—I'm updating my approach. These bots have different behaviors, and Google has explicitly said they respect robots.txt for AI training crawlers.</p>
        <p>Here's what I'm adding for clients concerned about their content being used to train AI models:</p>

        <pre><code>User-agent: GPTBot
Disallow: /

User-agent: ChatGPT-User
Allow: /

User-agent: CCBot
Disallow: /

User-agent: anthropic-ai
Disallow: /</code></pre>

        <p>The nuance here is important. GPTBot is for training data—block it if you don't want your content in future AI models. ChatGPT-User is for ChatGPT's browsing feature—allow it if you want ChatGPT to access your site when users ask it to.</p>
        <p>This is brand new territory. The rules are evolving monthly, and staying current is now part of my job description.</p>

        <h2>Building Your Own robots.txt Generator (Or What to Look For)</h2>
        
        <p>After seeing so many bad tools, I built a simple generator for my team. Here's what matters:</p>
        <ul>
          <li><strong>Pattern recognition:</strong> The tool should suggest blocking common paths based on your platform (WordPress, Shopify, custom).</li>
          <li><strong>Validation:</strong> It should check your syntax in real-time. One wrong slash and your whole site is blocked.</li>
          <li><strong>Testing integration:</strong> The best generators link directly to Google's robots.txt Tester so you can verify before deploying.</li>
          <li><strong>Version control:</strong> It should save previous versions because you will need to rollback at some point.</li>
          <li><strong>Comments:</strong> Good generators add comments explaining each rule, so six months from now you remember why you blocked that weird URL pattern.</li>
        </ul>
        <p>I also include a "commented out" section with rules I might need later—blocking a new marketing campaign's landing pages, for instance. It's easier to uncomment a line than to remember the exact syntax under pressure.</p>

        <h2>The Testing Protocol I Use Before Going Live</h2>
        
        <p>Never, ever, ever deploy a robots.txt file without testing. Here's my ritual:</p>
        <ol>
          <li><strong>Syntax check:</strong> Use a validator to catch basic errors.</li>
          <li><strong>Google's Tester:</strong> Put the URL in Search Console's robots.txt Tester. Google will tell you exactly which URLs are blocked and allowed.</li>
          <li><strong>Spot check:</strong> Manually test 10-15 critical URLs. Can I still access my homepage? My product pages? My blog posts?</li>
          <li><strong>Monitor for 24 hours:</strong> I deploy on a Tuesday morning and watch crawl stats like a hawk. If I see a sudden drop in pages crawled, I know I've made a mistake.</li>
          <li><strong>Backchannel check:</strong> I have a friend at another company with a different setup test a few URLs. Sometimes your own browser caches old rules.</li>
        </ol>
        <p>This might seem excessive, but remember my 2:47 AM email story. That client had made a "simple change" to their robots.txt and deployed it on a Friday evening. By Monday morning, they were invisible.</p>

        <h2>Your robots.txt Action Plan</h2>
        
        <p>If you're starting from scratch, here's my advice:</p>
        <ol>
          <li>Generate a basic file using a reputable tool. Keep it simple—block admin areas, allow everything else, point to your sitemap.</li>
          <li>Let it run for two weeks without touching it. Watch your crawl stats establish a baseline.</li>
          <li>Identify problems: Are bots crawling things they shouldn't? Are there bandwidth spikes? Are certain sections not getting indexed?</li>
          <li>Add surgical rules: One line at a time, to solve specific problems. Document why you added each rule.</li>
          <li>Review quarterly: Set a calendar reminder. Treat it like a health checkup.</li>
        </ol>
        <p>And if you're staring at an existing robots.txt file that looks like a digital archeology dig—layers of rules from five different developers over eight years? Sometimes the best solution is to scrap it and start fresh. I've done this twice for major sites. We backed up the old file, wrote a clean new one based on current site architecture, and saw immediate improvements in crawl efficiency.</p>

        <h2>The Bottom Line</h2>
        
        <p>Your robots.txt file is one of the few places in SEO where a tiny change can have massive consequences. It's not glamorous work. It doesn't make for exciting case studies. But get it right, and you're giving search engines a clear, efficient path to your best content. Get it wrong, and you're building a beautiful store with the doors welded shut.</p>
        <p>After fifteen years in this industry, I can tell you with absolute certainty: the technical fundamentals separate the professionals from the amateurs. Anyone can write content. Anyone can build links. But the people who master the invisible architecture—robots.txt, sitemaps, schema markup, site speed—they're the ones who win long-term.</p>
        <p>So take the time. Use a good generator as a starting point, but understand what it's creating. Test obsessively. Document compulsively. And for the love of all things holy, don't deploy on a Friday.</p>
      
      </div>

            <footer class="bg-gray-50 p-8 border-t">
        <div class="flex flex-col md:flex-row items-start md:items-center justify-between">
          <div class="mb-4 md:mb-0">
            <h3 class="font-semibold mb-2">About the Author</h3>
            <p class="text-sm text-gray-600 max-w-md">
              Alex Chen is a Technical SEO Specialist who has recovered dozens of sites from search penalties and crawling disasters.
            </p>
          </div>
          <a href="https://simpliconvert.com/tools/seo-analyzer/" class="bg-primary text-white px-6 py-3 rounded-lg hover:bg-primaryLight transition-colors font-semibold">
            Try Our SEO Analyzer →
          </a>
        </div>
      </footer>
    </article>

        <aside class="bg-gradient-to-r from-primary to-primaryLight text-white p-8 rounded-xl text-center my-12">
      <h3 class="text-3xl font-bold mb-4">Is Your Website Visible to Google?</h3>
      <p class="text-lg mb-6 opacity-90 max-w-2xl mx-auto">
        Don't let a simple technical error make your site invisible. Use our suite of SEO tools to analyze your site, check your robots.txt, and ensure you're perfectly optimized for search engines.
      </p>
      <a href="https://simpliconvert.com/tools/seo-analyzer/" class="bg-white text-primary px-8 py-4 rounded-lg font-bold hover:bg-gray-100 transition-colors inline-block text-lg">
        Analyze Your Site Now →
      </a>
    </aside>

        <section class="bg-white rounded-xl shadow-lg p-8 my-12">
      <h2 class="text-3xl font-bold mb-8 text-center">Frequently Asked Questions</h2>
      
      <div class="space-y-4">
        <details class="faq-item border-b border-gray-200 pb-4 group">
          <summary class="flex justify-between items-center cursor-pointer list-none font-semibold text-lg hover:text-primary">
            What is a robots.txt file?
            <span class="faq-icon transform transition-transform">▶</span>
          </summary>
          <div class="mt-3 text-gray-600 pl-6">
A<p>The robots.txt file is a set of instructions placed in your site's root directory. It tells web crawlers (like Googlebot) which pages or files on your site they are allowed to access and which ones they should skip. It's the first file a legitimate crawler reads.</p>
          </div>
        </details>

        <details class="faq-item border-b border-gray-200 pb-4 group">
          <summary class="flex justify-between items-center cursor-pointer list-none font-semibold text-lg hover:text-primary">
            Does robots.txt stop hackers or malicious bots?
            <span class="faq-icon transform transition-transform">▶</span>
          </summary>
          <div class="mt-3 text-gray-600 pl-6">
            <p>No. The robots.txt file is a suggestion, not a security lock. Legitimate crawlers (like Google's) will respect its rules, but malicious bots and hackers will almost always ignore it. It is an SEO tool, not a security measure.</p>
          </div>
        </details>

        <details class="faq-item border-b border-gray-200 pb-4 group">
          <summary class="flex justify-between items-center cursor-pointer list-none font-semibold text-lg hover:text-primary">
            How is blocking Googlebot different from blocking AI crawlers?
            <span class="faq-icon transform transition-transform">▶</span>
          </summary>
          <div class="mt-3 text-gray-600 pl-6">
            <p>Blocking Googlebot (e.g., <code>Disallow: /</code>) stops Google from indexing your site for search results, which is almost always a disaster. Blocking AI crawlers like GPTBot or CCBot prevents those companies from using your site content to train their AI models, which is a specific business decision and does not affect your Google search ranking.</p>
          </div>
        </details>

        <details class="faq-item pb-4 group">
          <summary class="flex justify-between items-center cursor-pointer list-none font-semibold text-lg hover:text-primary">
CSS          How often should I review my robots.txt file?
            <span class="faq-icon transform transition-transform">▶</span>
          </summary>
          <div class="mt-3 text-gray-600 pl-6">
            <p>You should treat it as a living document. It's recommended to review it quarterly, and always check it during and after any site redesign, platform update (like a WordPress update), or change in your site's URL structure.</p>
          </div>
        </details>
      </div>
    </section>

        <section class="my-12">
  TML   <h3 class="text-xl font-semibold mb-4">Related Topics</h3>
      <div class="flex flex-wrap gap-2">
        <a href="/blog/technical-seo-guide/" class="bg-gray-200 text-gray-700 px-4 py-2 rounded-full text-sm hover:bg-gray-300 transition-colors">technical seo</a>
        <a href="/tools/robots-txt-generator/" class="bg-gray-200 text-gray-700 px-4 py-2 rounded-full text-sm hover:bg-gray-300 transition-colors">robots.txt generator</a>
        <a href="/blog/google-crawl-budget/" class="bg-gray-200 text-gray-700 px-4 py-2 rounded-full text-sm hover:bg-gray-300 transition-colors">crawl budget</a>
        <a href="/blog/seo-best-practices/" class="bg-gray-200 text-gray-700 px-4 py-2 rounded-full text-sm hover:bg-gray-300 transition-colors">seo best practices</a>
      </div>
    </section>
  </main>
    <div id="footer"></div>

 <script>
    function initializeHeaderScripts() {
        // Header Scroll Effect
        const header = document.getElementById('header');
        if (header) {
            window.addEventListener('scroll', () => {
                if (window.scrollY > 10) {
    A                 header.classList.add('scrolled-effect');
                } else {
                    header.classList.remove('scrolled-effect');
                }
            });
        }

        // Mobile Menu Toggle
        const mobileMenuButton = document.getElementById('mobile-menu-button');
        const mobileMenu = document.getElementById('mobile-menu');
        const menuIconBars = document.getElementById('menu-icon-bars');
        const menuIconTimes = document.getElementById('menu-icon-times');
        
        if (mobileMenuButton && mobileMenu) {
            mobileMenuButton.addEventListener('click', () => {
                mobileMenu.classList.toggle('open');
                
                if (menuIconBars && menuIconTimes) {
                    menuIconBars.classList.toggle('hidden');
                    menuIconTimes.classList.toggle('hidden');
ci             }
            });
        }

        // Mobile Accordion Toggle
        const accordionButton = document.querySelector('.accordion-button');
        const accordionContent = document.querySelector('.accordion-content');
        
        if (accordionButton && accordionContent) {
            accordionButton.addEventListener('click', () => {
                accordionButton.classList.toggle('open');
s               accordionContent.classList.toggle('open');
            });
        }
    }
    
    function initializeFooterScripts() {
        // Set Current Year in Footer
        const yearSpan = document.getElementById('current-year');
        if(yearSpan) {
            yearSpan.textContent = new Date().getFullYear();
        }
    }

    async function loadComponent(url, elementId, callback) {
        try {
            const response = await fetch(url);
            if (!response.ok) {
                throw new Error(`Failed to load component from ${url}. Status: ${response.status}`);
img          }
            const text = await response.text();
            const element = document.getElementById(elementId);
            if (element) {
                element.innerHTML = text;
                if (callback) {
                    callback();
                }
            } else {
                console.error(`Element with id '${elementId}' not found.`);
            }
        } catch (error) {
            console.error('Error loading component:', error);
            const element = document.getElementById(elementId);
            if(element) element.innerHTML = `<p class="text-center text-red-500">Error: Could not load component.</p>`;
        }
    }

    document.addEventListener('DOMContentLoaded', function() {
        // Load Header and Footer components
        loadComponent('/components/navbar.html', 'navbar', initializeHeaderScripts);
        loadComponent('/components/footer.html', 'footer', initializeFooterScripts);
        
        // Smooth scrolling for anchor links
        document.addEventListener('click', function (e) {
            const anchor = e.target.closest('a[href^="#"]');
            if (!anchor) return;

            const href = anchor.getAttribute('href');
s           if (href === '#' || href === "") return;

            e.preventDefault();
            
            const targetElement = document.querySelector(href);
J           if (targetElement) {
                const header = document.getElementById('header');
                const headerOffset = header ? header.offsetHeight : 80;
                const elementPosition = targetElement.getBoundingClientRect().top;
                const offsetPosition = elementPosition + window.pageYOffset - headerOffset;
            
                window.scrollTo({
                    top: offsetPosition,
                    behavior: 'smooth'
                });

                // Close mobile menu if open after clicking a link
                const mobileMenu = document.getElementById('mobile-menu');
                if (mobileMenu && mobileMenu.classList.contains('open')) {
                    const mobileMenuButton = document.getElementById('mobile-menu-button');
                s     if(mobileMenuButton) mobileMenuButton.click();
                }
            }
        });

        // FAQ Accordion Logic
        const faqToggles = document.querySelectorAll('[data-faq-toggle]');
sv       faqToggles.forEach(button => {
            button.addEventListener('click', () => {
                const answer = button.nextElementSibling;
                const icon = button.querySelector('.faq-icon');
                const isOpening = !answer.style.maxHeight;

                // Close all other open FAQs
                faqToggles.forEach(otherButton => {
                    if (otherButton !== button) {
                        otherButton.nextElementSibling.style.maxHeight = null;
image                    otherButton.querySelector('.faq-icon').classList.remove('rotate-90');
                        otherButton.querySelector('.faq-icon').classList.add('-rotate-90');
                    }
                });

          s       // Toggle the clicked FAQ
                if (isOpening) {
                    answer.style.maxHeight = answer.scrollHeight + "px";
                    icon.classList.remove('-rotate-90');
                </span>     icon.classList.add('rotate-90');
                } else {
                    answer.style.maxHeight = null;
s                  icon.classList.remove('rotate-90');
                    icon.classList.add('-rotate-90');
                }
            });
        });
    });
</script>


  
</body>
</html>
